{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMZclcDv0s019ap9KWP6YmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huzaifa-dangote/open-source-llm-price-estimator/blob/main/QLoRA_Price_Estimator_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training an open source model with QLoRA to estimate product prices using text description\n",
        "\n",
        "If you are using LITE_MODE=True, then please run this on a free T4 box.\n",
        "\n",
        "If you are using LITE_MODE-False, then please use a paid A100 with high memory."
      ],
      "metadata": {
        "id": "L-9-wPVRoHPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade bitsandbytes==0.48.2 trl==0.25.1\n",
        "!wget -q https://raw.githubusercontent.com/ed-donner/llm_engineering/main/week7/util.py -O util.py"
      ],
      "metadata": {
        "id": "zIRmhddb6kTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nzMlnPHU7nWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constant\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
        "PROJECT_NAME = \"price\"\n",
        "HF_USER = \"huzaifa-dangote\" # your HF username here!\n",
        "\n",
        "LITE_MODE = True\n",
        "\n",
        "DATA_USER = \"ed-donner\"\n",
        "DATASET_NAME = f\"{DATA_USER}/items_prompts_lite\" if LITE_MODE else f\"{DATA_USER}/items_prompts_full\"\n",
        "\n",
        "RUN_NAME = f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "if LITE_MODE:\n",
        "  RUN_NAME += \"-lite\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# Hyper-parameters - overall\n",
        "\n",
        "#EPOCHS = 1 if LITE_MODE else 3\n",
        "EPOCHS = 3\n",
        "#BATCH_SIZE = 32 if LITE_MODE else 256\n",
        "BATCH_SIZE = 64\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "\n",
        "# Hyper-parameters - QLoRA\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "#LORA_R = 32 if LITE_MODE else 256\n",
        "LORA_R = 128\n",
        "LORA_ALPHA = LORA_R * 2\n",
        "ATTENTION_LAYERS = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "MLP_LAYERS = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "#TARGET_MODULES = ATTENTION_LAYERS if LITE_MODE else ATTENTION_LAYERS + MLP_LAYERS\n",
        "TARGET_MODULES = ATTENTION_LAYERS + MLP_LAYERS\n",
        "LORA_DROPOUT = 0.2\n",
        "\n",
        "# Hyper-parameters - training\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "WARMUP_RATIO = 0.01\n",
        "LR_SCHEDULER_TYPE = \"cosine\"\n",
        "WEIGHT_DECAY = 0.001\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "capability = torch.cuda.get_device_capability()\n",
        "use_bf16 = capability[0] >= 8\n",
        "\n",
        "# Tracking\n",
        "\n",
        "VAL_SIZE = 500 if LITE_MODE else 1000\n",
        "LOG_STEPS = 5 if LITE_MODE else 10\n",
        "SAVE_STEPS = 100 if LITE_MODE else 200\n",
        "LOG_TO_WANDB = True"
      ],
      "metadata": {
        "id": "7Qt5pfqK-pHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A100 supports this\n",
        "\n",
        "use_bf16"
      ],
      "metadata": {
        "id": "ddjVSlmZQwJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "vdOZGPfyRL2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to weights and biases\n",
        "\n",
        "wandb_api_key = userdata.get(\"WANDB_API_KEY\")\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure weights and biases to record our project\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"false\""
      ],
      "metadata": {
        "id": "fgR6dgItSRXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset[\"train\"]\n",
        "val = dataset[\"val\"].select(range(VAL_SIZE))\n",
        "test = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "VwOcvGAnVQkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ],
      "metadata": {
        "id": "kXpwGig2XFax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "      bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "      load_in_8bit=True,\n",
        "      bnb_8bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16\n",
        "  )"
      ],
      "metadata": {
        "id": "ktBF-gWPi-uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and the model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "_3isXCAyk2Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA parameters\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES\n",
        ")"
      ],
      "metadata": {
        "id": "NzCs2lam-_IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "\n",
        "training_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=LOG_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    fp16=not use_bf16,\n",
        "    bf16=use_bf16,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_length=MAX_SEQUENCE_LENGTH,\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=SAVE_STEPS\n",
        ")"
      ],
      "metadata": {
        "id": "FfjNXQvy__JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the trainer\n",
        "\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=val,\n",
        "    peft_config=lora_parameters,\n",
        "    args=training_parameters\n",
        ")"
      ],
      "metadata": {
        "id": "6gJqxrXVHMUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start fine-tuning\n",
        "\n",
        "fine_tuning.train()\n",
        "\n",
        "# Push fine-tuned model to Hugging Face\n",
        "\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "-70h44nTaeol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "biOfZNxXb64v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}