{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN5hWiG6xNESMvX/qL4DIv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huzaifa-dangote/open-source-llm-price-estimator/blob/main/QLoRA_Price_Estimator_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we evaluate our fine-tuned open source model"
      ],
      "metadata": {
        "id": "Fj4XOOMpPkJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lqLCUzcPdp9"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade bitsandbytes trl\n",
        "!wget -q https://raw.githubusercontent.com/ed-donner/llm_engineering/main/week7/util.py -O util.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from datetime import datetime\n",
        "from peft import PeftModel\n",
        "from util import evaluate"
      ],
      "metadata": {
        "id": "PQVq3uj-QGZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
        "PROJECT_NAME = \"price\"\n",
        "HF_USER = \"huzaifa-dangote\" # your HF name here\n",
        "\n",
        "LITE_MODE = True\n",
        "\n",
        "DATA_USER = \"ed-donner\"\n",
        "DATASET_NAME = f\"{DATA_USER}/items_prompts_lite\" if LITE_MODE else f\"{DATA_USER}/items_prompts_full\"\n",
        "\n",
        "if LITE_MODE:\n",
        "  RUN_NAME = \"2025-12-24_13.56.11-lite\"\n",
        "  REVISION = \"4a61d80fea82a75c2056363921b9e46375297759\"\n",
        "else:\n",
        "  RUN_NAME = \"\"\n",
        "  REVISION = None\n",
        "\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# Hyper-parameters - QLoRA\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "capability = torch.cuda.get_device_capability()\n",
        "use_bf16 = capability[0] >= 8"
      ],
      "metadata": {
        "id": "YU2wGLwMRfsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face login\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "X3cKHNIrGmaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "test = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "g_h71mzZHiQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[0]"
      ],
      "metadata": {
        "id": "wuf4qD2zIjjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "      bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "      load_in_8bit=True,\n",
        "      bnb_8bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16\n",
        "  )"
      ],
      "metadata": {
        "id": "-ZSjAgWVI8mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the fine-tuned model with PEFT\n",
        "if REVISION:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, HUB_MODEL_NAME, revision=REVISION)\n",
        "else:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, HUB_MODEL_NAME)\n",
        "\n",
        "\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "fQlkFgLDxJjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model"
      ],
      "metadata": {
        "id": "qEP4xtBBxeAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_predict(item):\n",
        "    inputs = tokenizer(item[\"prompt\"],return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = fine_tuned_model.generate(**inputs, max_new_tokens=8)\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    generated_ids = output_ids[0, prompt_len:]\n",
        "    return tokenizer.decode(generated_ids)"
      ],
      "metadata": {
        "id": "UG-fN8ywxex0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "evaluate(model_predict, test)"
      ],
      "metadata": {
        "id": "YbUyaQtzxiSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVlU_ehuyy1n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}